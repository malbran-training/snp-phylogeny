{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phylogeny from whole genome sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we sequence a population, we aim to capture the variation (SNPs, indels, gene gain and loss etc.) in the samples and use it to infer the relationships between the samples. Two of the main approaches to capturing this variation and reconstructing the bacterial genomes are:\n",
    "\n",
    "* De novo genome assembly and annotation\n",
    "* Mapping and variant calling against a reference genome\n",
    "\n",
    "Each approach has it's benefits and limitations. We will focus on mapping and variant calling in this tutorial. For mapping and variant calling, whether we are dealing with different bacterial isolates, with viral populations in a patient, or even with genomes of different human individuals, the principles are essentially the same. Instead of assembling the newly generated sequence reads de novo to produce a new genome sequence, it is easier and much faster to align or map the sequence reads to a reference genome. We can then readily identify SNPs and indels that distinguish closely related populations or individual organisms and may thus learn about genetic differences that may cause drug resistance or increased virulence in pathogens, or changed susceptibility to disease in humans. One important prerequisite for the mapping of sequence data to work is that the reference and the re-sequenced subject have the same genome architecture.\n",
    "\n",
    "In this exercise, we will use sequence data from _Salmonella enterica serovar Typhi_ samples to demonstrate the mapping and variant calling approach. Importantly, although the data is based on real sequence data, it has been edited to make it run more efficiently for the purpose of this tutorial.\n",
    "\n",
    "Navigate to the directory that contains the sequence data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/course_data/snp-phylogeny/data/typhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the directory containing the sequence data for the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the tutorial dataset\n",
    "\n",
    "We will use data adapted from the following paper:\n",
    "\n",
    "> **A genomic snapshot of Salmonella enterica serovar Typhi in Colombia**  \n",
    "> Guevara, Paula Diaz, et al.  \n",
    "> _PLoS Neglected Tropical Diseases2021. doi: 10.1371/journal.pntd.0009755_  \n",
    "> PMID: [34529660](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8478212/) \n",
    "\n",
    "_Salmonella enterica serovar Typhi_ (_S. Typhi_) is the causative agent of typhoid fever, with between 9–13 million cases and 116,800 associated deaths annually. Typhoid fever is still a public health problem in many countries, including in Latin America, which has a modelled incidence of up to 169 (32–642) cases per 100,000 person-years. Several international studies have aimed to fill data gaps regarding the global distribution and genetic landscape of typhoid; however, in spite of these efforts Latin America is still underrepresented. This study provided the first enhanced insights into the molecular epidemiology of S. Typhi in Colombia, using whole genome sequencing data to investigate the population structure in Colombia and identify predominant circulating genotypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of mapping and variant calling approach\n",
    "\n",
    "The diagram below illustrates the steps involved when mapping and calling variants for a set of bacterial samples.\n",
    "\n",
    "![Approach](img/snp-phylogeny-approach.png)\n",
    "\n",
    "The first step once you have obtained your sequence data (FASTQ) is to QC the data. After QC, the sequence data is matched or aligned to a reference genome (FASTA) in a process called read mapping to produce a set of read aligments (SAM/BAM). These read alignments are inspected to identify differences between the aligned reads and the reference genome. This process is called variant calling and produces VCF files. In fact during this process we capture information about every position in the genome (variant and non-variant sites) in the VCFs. Each site in the VCF has a set of quality filters applied and any sites identified as low quaility (e.g. less than 4 reads aligned at that position) are marked as low quality in the VCF to produce a filtered VCF. We use this filtered VCF file in a process called consensus caling to reconstruct a consensus _pseudosequence_ or _pseudogenome_ for our sample (FASTA). In the _pseudogenome_, any sites marked as low quality will be represented as an N in the reconstructed sequence. These pseudogenomes (multi-FASTA) are then aligned and variation identified and used to reconstruct a phylogeny of our samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Now let's analyse some data!\n",
    "\n",
    "### Prepare the data\n",
    "\n",
    "First take a look at the sequence data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls fastq/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your understanding\n",
    "\n",
    "1. How many samples have been sequenced?\n",
    "2. How many fastq files are there? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the chromosome sequence of _Salmonella typhi CT18_ as the reference genome. This has already been downloaded from RefSeq. Take a look at the reference genome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ref/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of the reference file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly-stats ref/Styphi_CT18.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use bwa to index the reference genome. This creates a lookup table that bwa uses when matching the sequence reads against the reference genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwa index ref/Styphi_CT18.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your understanding\n",
    "\n",
    "3. How many sequences in the reference fasta file?\n",
    "4. What are the names of the sequences in the reference fasta file?\n",
    "5. What is the size of the reference? \n",
    "6. What additional files did the indexing step produce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC the sequence data\n",
    "\n",
    "An important first step in any analysis is QC of the data. We will the FastQC software to QC the data. First create a directory for the qc results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd fastq\n",
    "mkdir qc_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run FastQC on the all the fastq files and store the results in the directory `qc_results`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastqc -o qc_results *.fastq.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create one html report for each of the fastq files. Take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls qc_results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have lots of samples it will be difficult to manually inspect each file. Therefore we will use multiQC to collate all the QC reports into one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiqc qc_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the collated report `multiqc_report.html` in firefox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firefox multiqc_report.html &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MultiQC results](img/multiqc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your understanding\n",
    "\n",
    "1. What is the median read length for sample ERR5243693?\n",
    "2. Which sample has the largets yield (mose sequence data)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim the reads to remove low quality and adapter sequence\n",
    "\n",
    "If your sequence reads have a high level of adapter contamination and/or have low quality bases at the end of the reads you can trim the reads to remove these sequences. There are several software that can be used to do this including trimmomatic and fastp.\n",
    "\n",
    "Use fastp to trim the reads for sample ERR5243693."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastp \\\n",
    "   --in1 ERR5243693_1.fastq.gz --in2 ERR5243693_2.fastq.gz \\\n",
    "   --out1 ERR5243693_1.trim.fastq.gz \\\n",
    "   --out2 ERR5243693_2.trim.fastq.gz \\\n",
    "   --json ERR5243693.fastp.json --html ERR5243693.fastp.html \\\n",
    "   --detect_adapter_for_pe --cut_mean_quality 20 \\\n",
    "   --thread 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat for the other samples.\n",
    "\n",
    "Take a look at the output from fastp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head -20 ERR5243693.fastp.json\n",
    "head -20 ERR5243695.fastp.json\n",
    "head -20 ERR5243699.fastp.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your understanding\n",
    "1. How much data (bp/base pairs) was lost due to trimming and adapter removal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the data to a reference genome "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing any data analysis it is good practice to arrange your data in a logical way rather than putting all files in a single directory. Let's create a directory to store the results of mapping the reads to the reference genome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir ../samtools\n",
    "cd ../samtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `bwa` to map the reads for sample ERR5243693 to the reference genome.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwa mem -t 2 ../ref/Styphi_CT18.fa \\\n",
    "../fastq/ERR5243693_1.trim.fastq.gz ../fastq/ERR5243693_2.trim.fastq.gz > \\\n",
    "ERR5243693.sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may take a few minutes to run. Let's take a look at the options:\n",
    "\n",
    "* `-t` :  tells the program to use 2 CPUs\n",
    "* `../ref/Styphi_CT18.fa` is the reference to match the reads against\n",
    "* `../fastq/ERR5243693_1.trim.fastq.gz and ../fastq/ERR5243693_2.trim.fastq.gz` are the fastq files containing our sequence reads (after trimming) for sample ERR5243693\n",
    "* the output is SAM format and `> ERR5243693.sam` redirects the output to the file `ERR5243693.sam`\n",
    "\n",
    "When complete, convert the sam file to a bam file with samtools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samtools view -@ 2 -bh -o ERR5243693.bam ERR5243693.sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `-@` option tells the program to use 2 CPUs and the `-b` option specifies to write the output as a bam file, `-h` option means include the header information in the output. The `-o` option specifies the name of the output file `ERR5243693.bam`.\n",
    "\n",
    "Sort the bam file and then index the sorted bam file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samtools sort -@ 1 -o ERR5243693.sorted.bam -T ERR5243693.sorted ERR5243693.bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samtools index ERR5243693.sorted.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some statistics about the alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samtools stats ERR5243693.sorted.bam > ERR5243693.stats\n",
    "samtools flagstat ERR5243693.sorted.bam > ERR5243693.flagstat\n",
    "samtools coverage ERR5243693.sorted.bam > ERR5243693.coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat for the other samples.\n",
    "\n",
    "#### Check your understanding\n",
    "1. What %reads mapped to the reference for each sample? \n",
    "2. What %genome was covered for each sample? \n",
    "3. What is the mean depth of coverage for each sample? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call variants\n",
    "\n",
    "Go through each position in the reference genome and look at reads aligned at that position and make a call about what the base is at that position for the sample. This information will be stored in a VCF file and if there are any differences then this will be marked as a variant (snp/indel) in this VCF file. \n",
    "\n",
    "Again create a directory to store the results of the variant calling step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir ../variants\n",
    "cd ../variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this for sample ERR5243693 using bcftools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcftools mpileup --fasta-ref ../ref/Styphi_CT18.fa \\\n",
    "--min-BQ 20 \\\n",
    "--annotate \\\n",
    "FORMAT/AD,FORMAT/ADF,FORMAT/ADR,FORMAT/DP,FORMAT/SP,INFO/AD,\\\n",
    "INFO/ADF,INFO/ADR ../samtools/ERR5243693.sorted.bam | bcftools call \\\n",
    "--output-type v --ploidy 1 --multiallelic-caller - | \\\n",
    "bcftools view --output-file ERR5243693.vcf.gz --output-type z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again this may take some time to run. Let's look at the options:\n",
    "\n",
    "The `bcftools mpileup` command is passed the following options:\n",
    "\n",
    "* `--fasta-ref ../ref/Styphi_CT18.fa` is the reference that the reads were mapped to\n",
    "* `--min-BQ 20` \n",
    "* `--annotate FORMAT/AD,FORMAT/ADF,FORMAT/ADR,FORMAT/DP,FORMAT/SP,INFO/AD,INFO/ADF,INFO/ADR` tells the program what tags to incude in the VCF\n",
    "\n",
    "The VCF produced by the bcftools mpileup command is passed to `bcftools call` with the following options:\n",
    "\n",
    "* `--output-type v` tells the program to output VCF format\n",
    "* `--ploidy 1` tells the program that we are daling with haploid datasets\n",
    "* `--multiallelic-caller` tells the program which variant calling algorithm to use\n",
    "\n",
    "The VCF produced by the `bcftools call` command is passed to `bcftools view` with the following options which converts the VCF to a compressed VCF file:\n",
    "\n",
    "* `--output-file ERR5243693.vcf.gz` tells the program the name of the output file\n",
    "* `--output-type z` tells the program to create a gzipped VCF file as output\n",
    "\n",
    "When it completes, index the gzipped VCF file that has been created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabix -p vcf -f ERR5243693.vcf.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the VCF file that was produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcftools view ERR5243693.vcf.gz | less -S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the VCF has information about every site in the genome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate some statistics about the VCF file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcftools stats ERR5243693.vcf.gz > ERR5243693.vcf.stats.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat for the other samples.\n",
    "\n",
    "Look at the statistics for the variant calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less ERR5243693.vcf.stats.txt\n",
    "less ERR5243695.vcf.stats.txt\n",
    "less ERR5243699.vcf.stats.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your understanding\n",
    "1. How many sites are in the VCF file for each sample?\n",
    "2. Does this match to the size of the reference used in the read mappping step?\n",
    "3. How many variant sites were identified for each sample?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the VCFs\n",
    "We want to identify calls where we have a high confidence that they are correct (and not due to sequencing errors and/or misalignment of the reads). We use criteria like read depth at a position, quality scores etc. to filter out low quality calls at each position.\n",
    "\n",
    "Use bcftools to filter sites for sample ERR5243693."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcftools filter \\\n",
    "--output ERR5243693.filtered.vcf.gz \\\n",
    "--soft-filter LowQual \\\n",
    "--exclude \"QUAL<25 || FORMAT/DP<10 || MAX(FORMAT/ADF)<2 || MAX(FORMAT/ADR)<2 \\\n",
    "|| MAX(FORMAT/AD)/SUM(FORMAT/DP)<0.9 || MQ<30 || MQ0F>0.1\" \\\n",
    "--output-type z ERR5243693.vcf.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtering step may take some time to run. Let's look at the options used in the command above:\n",
    "\n",
    "* `--output` specifies the name of the output file\n",
    "* `--soft-filter` tells the program to keep any filtered position in the file and mark them as `LowQual` rather than remove them completely from the file (hard filter)\n",
    "* `--exclude` lists the filtering criteria to apply to each position\n",
    "* `--output-type` specifies the type of output file to create, in this case z means a xompressed VCF file\n",
    "\n",
    "When the bcftools filter command is complete, index the filtered VCF file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabix -p vcf -f ERR5243693.filtered.vcf.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the VCF file and notice how some of the sites are marked as `PASS` or `LowQual` under the filter column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcftools view ERR5243693.filtered.vcf.gz | less -S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your understanding\n",
    "\n",
    "Looking at the filtered VCF for sample ERR5243693:\n",
    "\n",
    "1. Does position 2 pass or fail?  \n",
    "2. Does position 244 pass or fail?  \n",
    "3. What is the reason for the failure at position 2527?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view only the sites that passed the filtering step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcftools view -f PASS ERR5243693.filtered.vcf.gz | less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate some statistics about the filtered VCF file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcftools stats ERR5243693.filtered.vcf.gz > \\\n",
    "ERR5243693.filtered.stats.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat for the other samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your understanding\n",
    "4. How many sites were marked as low quality in the filtering step?\n",
    "5. How many variant sites were marked as low quality in the filtering step?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call a consensus sequence for each sample\n",
    "\n",
    "A pseudogenome is a reconstruction of what we think the genome is for the sample using the reference genome as a basis. To create it for a sample, you go through each position in the reference and determine what base is called (using the VCF from the previous steps) for the sample. Sometimes this will be the same as the reference, and sometimes it will differ from the reference (a variant). For positions that are flagged as low quality/filtered out (e.g. no reads covering the position) we use an N in the pseudogenome. This is because you cannot be confident what the base is at this position for the sample. In the end the length of the pseudogenome for your sample should be the same as the length of the reference. \n",
    "\n",
    "To create a pseudogenome for sample ERR5243693 use the script _vcf2pseudogenome.pl_. This has already been installed on the computer using this command (it also has a dependency on _pysam_ and _biopython_):\n",
    "\n",
    "`wget https://raw.githubusercontent.com/nf-core/bactmap/master/bin/vcf2pseudogenome.py`\n",
    "\n",
    "Create a directory to store the pseudogenomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir ../pseudogenomes\n",
    "cd ../pseudogenomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run the `vcf2pseudogenome.py` script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf2pseudogenome.py -r ../ref/Styphi_CT18.fa \\\n",
    "-b ../variants/ERR5243693.filtered.vcf.gz -o ERR5243693.fas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat for the other samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your understanding\n",
    "1. What is the length of the pseudogenomes? (Hint: Use assembly-stats)\n",
    "2. Does it match the length of the reference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a multiple sequence alignment of all pseudogenomes\n",
    "Remember that to reconstruct the phylogeny of our samples we need a multifasta alignment of our sequences. We are going to use a reconstruction of the entire genome as our set of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat *.fas > aligned_pseudogenomes.aln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the way the pseudogenomes were constructed resulting in them all being the same length we do not have to perform a multi-sequence alignment to ensure all the bases are homologous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your understanding\n",
    "1. How many sequences in the multiple sequence alignment file of pseudogenomes?\n",
    "2. What is the largest and mean sequence length? (Hint: Use assembly-stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the reference genome to the multiple sequence alignment\n",
    "Sometimes you may want to include the reference genome in your tree. To do this, add the reference sequence to the multifasta alignment of your samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ../ref/Styphi_CT18.fa aligned_pseudogenomes.aln \\\n",
    "> ref_and_aligned_pseudogenomes.aln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it would be useful to look at your alignment in a multiple sequence alignment viewer e.g. seaview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaview ref_and_aligned_pseudogenomes.aln &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your understanding\n",
    "1. How many sequences in the multiple sequence alignment file containng the reference genome and sample pseudogenomes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask repetitive regions (optional but good practice)\n",
    "Bases called in repetitive regions may not be true variation (e.g. due to misalignment of reads) and may compromise the core phylogenetic signal. Therefore it is good practice to identify known repetitive regions and mask these out from your alignment.\n",
    "\n",
    "To achieve this, either a file of known regions for the reference you mapped to will exist (see the literature) or one can be generated by matching the reference genome against itself (to identify repeat regions) with e.g. Mummer and Phast used to identify prophage. You can use the software remove_blocks to mask out any repetitive region in your multifasta alignment, this masking involves replacing the bases with Ns in this region. To save time we wont run this step here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw a tree with iqtree\n",
    "Now that we have a multiple sequence alignment, we can use IQ-TREE to build a maximum likelihood phylogeny. But first create a directory to store the iqtree analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir ../iqtree\n",
    "cd ../iqtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating a phylogeny on whole genome sequences can be very time consuming. We can speed this up by only using the variable sites (SNPs). These are sites that differ in at least one of the samples. However, we need to be aware that only including variable sites can affect the evolutionary rate estimates made by phylogenetics software - therefore, we need to account for the sites we remove in our analysis.\n",
    "\n",
    "We will use snp-sites to do this. You can view the options for snp-sites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp-sites -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, remove all the invariant sites and create a SNP-only multiple sequence alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp-sites -o pseudogenomes.snps.aln \\\n",
    "../pseudogenomes/ref_and_aligned_pseudogenomes.aln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the new file that is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less pseudogenomes.snps.aln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how many invariant sites were removed (and what proportion of A, T, G, C they were) using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp-sites -C ../pseudogenomes/ref_and_aligned_pseudogenomes.aln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your understanding\n",
    "\n",
    "1. How many variant sites are identified?\n",
    "2. How many invariant sites are identified?\n",
    "3. Does this correlate with the expected number i.e. from the literature?\n",
    "\n",
    "Now look at the options for IQ-TREE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqtree -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then construct the tree with IQ-TREE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqtree \\\n",
    "    -s pseudogenomes.snps.aln \\\n",
    "    -fconst $( snp-sites -C ../pseudogenomes/ref_and_aligned_pseudogenomes.aln ) \\\n",
    "    -m GTR+G+T \\\n",
    "    -B 1000 \\\n",
    "    -T 2 \\\n",
    "    -mem 2GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the command above, we have used te following options\n",
    "\n",
    "* `-s` to specify the multiple sequence alignment file `pseudogenomes.snpsites.aln`\n",
    "* `-fconst` to ask IQ-TREE to take account of missing invariant sites $(snp-sites -C ref_and_aligned_pseudogenomes.aln calculates the specific values to pass to iqtree)\n",
    "* `-m` to specify an evolutionary model, we want IQ-TREE to use -m GTR\n",
    "* `-B` to perform 1000 ultrafast bootstraps \n",
    "* `-T` to tell IQ-TREE to use a maximum of 2 CPUs (threads) \n",
    "* `-mem` 2GB to tell IQ-TREE to use a maximum of of 2GB memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our maximum likelihood tree is labelled `pseudogenomes.snpsites.aln.treefile`. The treefile suffix is not always correctly identified by many tools, so we'll relabel this as something else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp pseudogenomes.snps.aln.treefile pseudogenomes.snps.aln.tre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the raw text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat pseudogenomes.snps.aln.tre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it is better if we visualise this using a tree view e.g.figtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figtree pseudogenomes.snps.aln.tre &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accounting for recombination with Gubbins\n",
    "\n",
    "Variation due to recombination events can mask the core phylogenetic signal, therefore it is recommended to identify these regions in your alignment and mask them out. We can use gubbins to infer recombining sites by looking for increased SNP density that occurs in specific ancestral nodes. Look at the options for gubbins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_gubbins.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a directory to store the results of running gubbins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir ../gubbins\n",
    "cd ../gubbins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run gubbins on your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_gubbins.py -c 2 -p gubbins \\\n",
    "../pseudogenomes/ref_and_aligned_pseudogenomes.aln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `-c` option tells the program to use 2 CPUs and the `-p` option tells the program to name all files with the prefix gubbins. This command can take a few minutes to run.\n",
    "\n",
    "__Note:__ Gubbins must be run on the entire alignment (not just SNPs) as it uses the spatial distribution to identify and filter out recombinant regions.\n",
    "\n",
    "NB. If gubbins takes more than 10 mins to complete, we have already run it for you - the files are available at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l ~/course_data/snp-phylogeny/data/typhi/gubbins_backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at what gubbins has done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore these files. For example `gubbins.recombination_predictions.gff` is a gff file that contains a record of each recombination block identified, how many SNPs it contains, and what samples are affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "head gubbins.recombination_predictions.gff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The file `gubbins.final_tree.tre` is a phylogeny in which recombination has already been accounted for.\n",
    "\n",
    "You can visualise this in figtree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figtree gubbins.final_tree.tre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root the phylogeny\n",
    "\n",
    "The trees generated by iqtree and gubbins are unrooted, but we may want to apply some evolutionary direction to them. One strategy for rooting a tree is called _midpoint rooting_. Midpoint rooting involves locating the midpoint of the longest path between any two tips and putting the root in that location. Note that this does not necessarily infer the true root, and this should be used with caution.\n",
    "\n",
    "To midpoint root our tree, we will use a simple script written in python that uses the ete package. You can examine the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less ../midpoint.root.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Python script to midpoint root a tree](img/midpoint.root.image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this script to midpoint root the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python midpoint.root.py gubbins.final_tree.tre > gubbins.final_tree.midpoint.tre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise this in figtree.\n",
    "\n",
    "Another common strategy for rooting the tree is _outgroup rooting_. This is the preferred approach for bacterial datasets. Outgroup rooting involves including one or more sequences in the analysis that are more distantly related to our sequences of interest than they are to one another. These sequence are usually referred to as _outgroups_. The root estimate is then simply the point at which the outgroup(s) join the tree. The best possible outgroups are those available which are most closely related to our sequences of interest but still different enough. For examples, in this dataset we could use a _Salmonella paratyhi A sample_ as an outgroup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few ways to do this. One is to obtain sequence data for the outgroup sample and incorporate it into the dataset from the beginning of the analysis and construct a pseudogenome for the outgroup. If no sequence data is available, you can take a complete reference genome for the outgroup and `shred` it to simulate sequencing reads for the reference. Just remember when calculating and reporting the number of variant sites for your dataset that you remove the outgroup from the alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up!\n",
    "\n",
    "Clean up any intermediate files that were generated during the analysis that you no longer require. This is always an important last step of any analysis as sequence data analysis files can use up large amounts of disk space. Let's look at what files were created in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ..\n",
    "ls -alhrt */*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the data analysis has generated a lot of files! So let's remove any files that do not need to be kept long term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm fastq/*.trim.fastq.gz\n",
    "rm fastq/*.fastp.html\n",
    "rm fastq/*.fastp.json\n",
    "rm samtools/*.sam\n",
    "rm samtool/ERR5243693.bam*\n",
    "rm samtools/ERR5243695.bam*\n",
    "rm samtools/ERR5243699.bam*\n",
    "rm variants/ERR5243693.vcf.gz*\n",
    "rm variant/ERR5243695.vcf.gz*\n",
    "rm variants/ERR5243699.vcf.gz*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go to the next section: [Phylogeny and Metadata](metadata.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
